{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9aef7a",
   "metadata": {},
   "source": [
    "# üìö Understanding the OpenAssistant OASST1 Dataset\n",
    "\n",
    "This notebook explains the OASST1 dataset used for Reinforcement Learning from Human Feedback (RLHF) training pipelines. We'll cover its origin, structure, content, and purpose.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3d423",
   "metadata": {},
   "source": [
    "## ‚úÖ What is the OASST1 Dataset?\n",
    "\n",
    "The **OpenAssistant (OASST1)** dataset is an open-source collection of human-generated dialogues, used primarily for training conversational AI systems. It was created as part of the OpenAssistant project, aiming to democratize large language model development and make RLHF accessible.\n",
    "\n",
    "- **Source**: [OpenAssistant Project on Hugging Face](https://huggingface.co/datasets/OpenAssistant/oasst1)\n",
    "- **Primary Use**: Fine-tuning language models, especially for RLHF pipelines.\n",
    "- **Data Format**: JSON-based dialogue format, accessible via Hugging Face Datasets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ba0787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../data/oasst1_small\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13e2b8",
   "metadata": {},
   "source": [
    "## üîç Dataset Structure and Format\n",
    "The dataset is structured as a list of conversation samples. Each entry includes:\n",
    "\n",
    "- `text`: A string containing the conversation or prompt-response dialogue.\n",
    "- `role`: Specifies who generated the content (human or AI assistant).\n",
    "\n",
    "Let's explore a sample entry to understand better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2779ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The role of the sample is:\n",
      "prompter\n",
      "\n",
      "The text in the sample is:\n",
      "Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display a single sample\n",
    "sample = dataset[0]\n",
    "print(f\"The role of the sample is:\\n{sample['role']}\\n\")\n",
    "print(f\"The text in the sample is:\\n{sample['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55d421",
   "metadata": {},
   "source": [
    "## üìä Dataset Size\n",
    "\n",
    "Let's check the total size of this small subset for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f53dcce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(dataset)} samples in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bee994",
   "metadata": {},
   "source": [
    "## üéØ Use in RLHF (Reinforcement Learning with Human Feedback)\n",
    "\n",
    "The primary use of the OASST1 dataset in RLHF involves:\n",
    "\n",
    "1. **Fine-tuning**: Initially fine-tuning a language model (e.g., GPT-2, GPT-3) on conversational data.\n",
    "2. **Reward Modeling**: Training a reward model that predicts human preferences.\n",
    "3. **Reinforcement Learning (PPO)**: Optimizing the language model using RL (like PPO) to maximize predicted human preferences.\n",
    "\n",
    "This dataset specifically helps at the initial fine-tuning stage, and the text-based dialogues are ideal for generating reward signals.\n",
    "\n",
    "## üìå Next Steps\n",
    "In the next step, we'll use this dataset to build a basic RLHF pipeline, then profile and optimize performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd62fbc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
